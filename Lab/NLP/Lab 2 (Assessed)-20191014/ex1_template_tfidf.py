# -*- coding: utf-8 -*-
"""ex1_template.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1WtuJ6XLmc1hvpIkOdyiU9AKT4hyo7MnZ
"""

from __future__ import print_function  # needed for Python 2
from __future__ import division		# needed for Python 2
import csv,re,sys
import collections, itertools
import numpy as np
import pandas as pd
from scipy import stats							   # csv reader
from sklearn.svm import LinearSVC
from nltk.classify import SklearnClassifier
from random import shuffle
from sklearn.pipeline import Pipeline
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
stopwords = stopwords.words('english')
porter = PorterStemmer()
# !pip install nltk
# !pip install sklearn
import nltk, math
# nltk.download('stopwords')
# nltk.download('words')

# load data from a file and append it to the rawData
digit_contractions = {
	'hr' : 'hour',
	'yr' : 'year',
	'mm' : 'millimeter',
	'mb' : 'megabit',
	'lb' : 'pound',
	'kb' : 'kilobit',
	'kg' : 'kilogram',
	'am' : 'morning',
	'pm' : 'aftrnoon',
	'mg' : 'milligram',
	'px' : 'pixel',
	'ft' : 'feet',
	'cm' : 'centimeter',
	'in' : 'inch',
	'p' : 'pixel',
	'hd' : 'high definition',
	'mp' : 'megapixel',
	'gb' : 'gigabit',
	'oz' : 'pound',
	'hz' : 'hertz',
	'th' : 'day',
	'st' : 'day',
	'nd' : 'day',
	'rd' : 'day',
	'hp' : 'horsepower',
	'tb' : 'terabit',
	'pc' : 'computer',
	'dr' : 'doctor',
	'k' : 'kilo',
	'i' : 'inch',
	'ml' : 'mililiter',
	'gm' : 'gram',
	'km' : 'kilometer',
	'mn' : 'million'
}
general_contraction = {
	"ain't": 'is not',
 "aren't": 'are not',
 "can't": 'cannot',
 "can't've": 'cannot have',
 "'cause": 'because',
 "could've": 'could have',
 "couldn't": 'could not',
 "couldn't've": 'could not have',
 "didn't": 'did not',
 "doesn't": 'does not',
 "don't": 'do not',
 "hadn't": 'had not',
 "hadn't've": 'had not have',
 "hasn't": 'has not',
 "haven't": 'have not',
 "he'd": 'he would',
 "he'd've": 'he would have',
 "he'll": 'he will',
 "he'll've": 'he he will have',
 "he's": 'he is',
 "how'd": 'how did',
 "how'd'y": 'how do you',
 "how'll": 'how will',
 "how's": 'how is',
 "I'd": 'I would',
 "I'd've": 'I would have',
 "I'll": 'I will',
 "I'll've": 'I will have',
 "I'm": 'I am',
 "I've": 'I have',
 "i'd": 'i would',
 "i'd've": 'i would have',
 "i'll": 'i will',
 "i'll've": 'i will have',
 "i'm": 'i am',
 "i've": 'i have',
 "isn't": 'is not',
 "it'd": 'it would',
 "it'd've": 'it would have',
 "it'll": 'it will',
 "it'll've": 'it will have',
 "it's": 'it is',
 "let's": 'let us',
 "ma'am": 'madam',
 "mayn't": 'may not',
 "might've": 'might have',
 "mightn't": 'might not',
 "mightn't've": 'might not have',
 "must've": 'must have',
 "mustn't": 'must not',
 "mustn't've": 'must not have',
 "needn't": 'need not',
 "needn't've": 'need not have',
 "o'clock": 'of the clock',
 "oughtn't": 'ought not',
 "oughtn't've": 'ought not have',
 "shan't": 'shall not',
 "sha'n't": 'shall not',
 "shan't've": 'shall not have',
 "she'd": 'she would',
 "she'd've": 'she would have',
 "she'll": 'she will',
 "she'll've": 'she will have',
 "she's": 'she is',
 "should've": 'should have',
 "shouldn't": 'should not',
 "shouldn't've": 'should not have',
 "so've": 'so have',
 "so's": 'so as',
 "that'd": 'that would',
 "that'd've": 'that would have',
 "that's": 'that is',
 "there'd": 'there would',
 "there'd've": 'there would have',
 "there's": 'there is',
 "they'd": 'they would',
 "they'd've": 'they would have',
 "they'll": 'they will',
 "they'll've": 'they will have',
 "they're": 'they are',
 "they've": 'they have',
 "to've": 'to have',
 "wasn't": 'was not',
 "we'd": 'we would',
 "we'd've": 'we would have',
 "we'll": 'we will',
 "we'll've": 'we will have',
 "we're": 'we are',
 "we've": 'we have',
 "weren't": 'were not',
 "what'll": 'what will',
 "what'll've": 'what will have',
 "what're": 'what are',
 "what's": 'what is',
 "what've": 'what have',
 "when's": 'when is',
 "when've": 'when have',
 "where'd": 'where did',
 "where's": 'where is',
 "where've": 'where have',
 "who'll": 'who will',
 "who'll've": 'who will have',
 "who's": 'who is',
 "who've": 'who have',
 "why's": 'why is',
 "why've": 'why have',
 "will've": 'will have',
 "won't": 'will not',
 "won't've": 'will not have',
 "would've": 'would have',
 "wouldn't": 'would not',
 "wouldn't've": 'would not have',
 "y'all": 'you all',
 "y'all'd": 'you all would',
 "y'all'd've": 'you all would have',
 "y'all're": 'you all are',
 "y'all've": 'you all have',
 "you'd": 'you would',
 "you'd've": 'you would have',
 "you'll": 'you will',
 "you'll've": 'you will have',
 "you're": 'you are',
 "you've": 'you have',
 "b'day": 'birthday',
 'aint': 'is not',
 'arent': 'are not',
 'cant': 'cannot',
 'cantve': 'cannot have',
 'couldve': 'could have',
 'couldnt': 'could not',
 'couldntve': 'could not have',
 'didnt': 'did not',
 'doesnt': 'does not',
 'dont': 'do not',
 'hadnt': 'had not',
 'hadntve': 'had not have',
 'hasnt': 'has not',
 'havent': 'have not',
 'howd': 'how did',
 'howdy': 'how do you',
 'howll': 'how will',
 'hows': 'how is',
 'Id': 'I would',
 'Idve': 'I would have',
 'Ill': 'I will',
 'Illve': 'I will have',
 'Im': 'I am',
 'Ive': 'I have',
 'id': 'i would',
 'idve': 'i would have',
 'ill': 'i will',
 'illve': 'i will have',
 'im': 'i am',
 'ive': 'i have',
 'isnt': 'is not',
 'itd': 'it would',
 'itdve': 'it would have',
 'itll': 'it will',
 'itllve': 'it will have',
 'its': 'it is',
 'lets': 'let us',
 'maam': 'madam',
 'maynt': 'may not',
 'mightve': 'might have',
 'mightnt': 'might not',
 'mightntve': 'might not have',
 'mustve': 'must have',
 'mustnt': 'must not',
 'mustntve': 'must not have',
 'neednt': 'need not',
 'needntve': 'need not have',
 'oclock': 'of the clock',
 'oughtnt': 'ought not',
 'oughtntve': 'ought not have',
 'shant': 'shall not',
 'shantve': 'shall not have',
 'shed': 'she would',
 'shedve': 'she would have',
 'shell': 'she will',
 'shellve': 'she will have',
 'shes': 'she is',
 'shouldve': 'should have',
 'shouldnt': 'should not',
 'shouldntve': 'should not have',
 'sove': 'so have',
 'sos': 'so as',
 'thatd': 'that would',
 'thatdve': 'that would have',
 'thats': 'that is',
 'thered': 'there would',
 'theredve': 'there would have',
 'theres': 'there is',
 'theyd': 'they would',
 'theydve': 'they would have',
 'theyll': 'they will',
 'theyllve': 'they will have',
 'theyre': 'they are',
 'theyve': 'they have',
 'tove': 'to have',
 'wasnt': 'was not',
 'wed': 'we would',
 'wedve': 'we would have',
 'wellve': 'we will have',
 'were': 'we are',
 'weve': 'we have',
 'werent': 'were not',
 'whatll': 'what will',
 'whatllve': 'what will have',
 'whatre': 'what are',
 'whats': 'what is',
 'whatve': 'what have',
 'whens': 'when is',
 'whenve': 'when have',
 'whered': 'where did',
 'wheres': 'where is',
 'whereve': 'where have',
 'wholl': 'who will',
 'whollve': 'who will have',
 'whos': 'who is',
 'whove': 'who have',
 'whys': 'why is',
 'whyve': 'why have',
 'willve': 'will have',
 'wont': 'will not',
 'wontve': 'will not have',
 'wouldve': 'would have',
 'wouldnt': 'would not',
 'wouldntve': 'would not have',
 'yall': 'you all',
 'yalld': 'you all would',
 'yalldve': 'you all would have',
 'yallre': 'you all are',
 'yallve': 'you all have',
 'youd': 'you would',
 'youdve': 'you would have',
 'youll': 'you will',
 'youllve': 'you will have',
 'youre': 'you are',
 'youve': 'you have',
 ' ootd': 'outfit of the day',
}
def loadData(path, Text=None):
	with open(path,'r') as f:
		reader = csv.reader(f, delimiter='\t')
		# reader.next() # ignore header
		next(reader)
		for line in reader:
			(Id, Text, Rating, VerPurchase, Label) = parseReview(line)
			rawData.append((Id, Text, Rating, VerPurchase, Label))
			preprocessedData.append((Id, preProcess(Text), Rating, VerPurchase, Label))


		
def splitData(percentage):
	dataSamples = len(rawData)
	halfOfData = int(len(rawData)/2)
	trainingSamples = int((percentage*dataSamples)/2)
	for (index, Text,_,_, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:
		trainData.append((toFeatureVector(preProcess(Text),index),Label))
	for (index, Text,_,_, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:
		testData.append((toFeatureVector(preProcess(Text),index),Label))

# QUESTION 1

# Convert line from input file into an id/text/label tuple
def parseReview(reviewLine):
	"""
	reviewLine is a list
	"""
	# Should return a triple of an integer, a string containing the review, and a string indicating the label
	doc_id = int(reviewLine[0])
	label = reviewLine[1]
	review_text = reviewLine[8]
	raiting = reviewLine[2]
	verPurchase = reviewLine[3]
	return (doc_id, review_text, raiting, verPurchase ,label)

# TEXT PREPROCESSING AND FEATURE VECTORIZATION
# Input: a string of one review
def preProcess(text):
	# Should return a list of tokens	
	# CHANGE THE CODE BELOW
	text = text.lower()
	# remove speacial char including all puncuattions and replace it with a space
	for k,v in general_contraction.items():
		if k in text.split():
			text = text.replace(k,v)
	text = re.sub('[^A-Za-z0-9]+',' ',text)
	# tokenise
	tokens = text.split()
	# stop word removal
	tokens = [w for w in tokens if w not in stopwords]
	# Stemming
	tokens = [str(porter.stem(w)) for w in tokens]
	# if word is non-english return its english form # too much time-complexity
	# tokens = [porter.stem(w) if porter.stem(w) in set(words.words()) else w for w in tokens ]
	# for words having digits such as 12gb, 1st, etc expanding the token list
	for k in tokens:
		if len(k) >2 and re.match(r'[0-9]+',k):			
			if len(k) >2 and not k.isdigit():
				l = re.split(r'(\d+)',k)
				l = [w for w in l if w is not '' ]
				if l and len(l) <= 3:
					for i in l:
						if i in digit_contractions.keys():
							l = list(map(lambda b: b.replace(i,digit_contractions[i]), l))
					# print(str(l),k)
					tokens.remove(k)
					tokens = tokens+l
				else:
					# print('removing',k)
					tokens.remove(k)
	for k,v in digit_contractions.items():
		if k in tokens:
			if tokens[tokens.index(k)-1].isdigit():	
				# print(tokens[tokens.index(k)-1],k,v)			
				tokens = list(map(lambda b: b.replace(k,v), tokens))
				# print(tokens[tokens.index(k)-1],k,v)
	return tokens

# QUESTION 2
rawData = []
preprocessedData = []
loadData('amazon_reviews.txt')
counterDict = {} 
alltokens =[]
for i in preprocessedData:
	alltokens.extend(i[1])

counterDict = collections.Counter(alltokens)
mostCommonWord = counterDict.most_common(10)
print('Most Common word',mostCommonWord)

if mostCommonWord[0][0] == 'br':
	print('adding '+mostCommonWord[0][0]+' to stopwords and removing it')
	stopwords.append('br')
	counterDict.pop('br')
stopwordNew=[]
# for k,v in counterDict.items():
# 	if len(k) <=2 and not k.isdigit():

# 		stopwordNew.append(k)
	
print('adding below to stopwords and removing it')
print(str(stopwordNew))
for w in stopwordNew:
	counterDict.pop(w)
stopwords = stopwords+stopwordNew
#print(sorted(counterDict.items(), key=lambda pair: pair[1], reverse=False)[0:50])

featureDict = {} # A global dictionary of features
total_no_of_clean_words = sum(counterDict.values())
total_no_of_reviews = len(rawData)
print("Total Number of words: "+str(total_no_of_clean_words))
print("Total Number of reviews: "+str(total_no_of_reviews))

for k,v in counterDict.items():
	featureDict[k] = (float(v)/total_no_of_clean_words)*math.log(float(total_no_of_reviews) / v)

print("Built feature dict")

length_of_token = []
for i in preprocessedData:
	length_of_token.append(len(i[1]))

mean_token = int(np.mean(length_of_token))
median_token = int(np.median(length_of_token))
print("Avg length of tokens", mean_token)
print("Median of tokens",median_token)
print("Mode of tokens",stats.mode(length_of_token))


def toFeatureVector(tokens,index=None):
	# Should return a dictionary containing features as keys, and weights as values
	'''
	Average word length
	Number of stopwords
	Number of special characters
	Number of numerics


	'''
	adict = {}
	tokens = [w for w in tokens if w not in stopwords]
	#print(tokens,index)
	for i in tokens[:median_token]:
		adict[i] = featureDict[i]
	if index is not None:
		for i in rawData:
			if i[0] == index:
				adict['raiting'] = i[2]
				adict['verPur'] = 1 if i[3] == 'Y' else 0
				adict['avgWordLen'] = sum(len(w) for w in i[1].split())/len(i[1])
				adict['stopwords'] = len([w for w in i[1].split() if w in stopwords])
				# adict['speacialChar'] = len(re.findall(r'[^A-Z0-9a-z ]+',i[1]))
				adict['digits'] = len(re.findall(r'[0-9]+',i[1]))

	#print(adict)
	return adict

# TRAINING AND VALIDATING OUR CLASSIFIER
def trainClassifier(trainData):
	print("Training Classifier...")
	pipeline =  Pipeline([('svc', LinearSVC())])
	return SklearnClassifier(pipeline).train(trainData)

# QUESTION 3

def crossValidate(dataset, folds):
	shuffle(dataset)
	cv_results = []
	precision_recall_acc = []

	foldSize = int(len(dataset)/folds)
	for i in range(0,len(dataset),foldSize):
		valD = dataset[i:i+foldSize]
		testD = dataset[:i]+dataset[i+foldSize:] #list(set(dataset)-set(dataset[i:i+foldSize]))
		print("Training on data-set size "+str(len(testD))+" of batch "+str(i/len(testD)))
		classi = trainClassifier(testD)
		print("Predicting on heldout data-set size..."+str(len(valD))+" of batch "+str(i))
		y_true = list(map(lambda t: t[1], valD))
		y_pred = predictLabels(valD,classi)
		
		precision_recall = precision_recall_fscore_support(y_true, y_pred, average='macro')
		acc = accuracy_score(y_true,y_pred)
		precision_recall = list(precision_recall)
		precision_recall[-1] = acc
		print(precision_recall)
		precision_recall_acc.append(precision_recall)
	df = pd.DataFrame(precision_recall_acc)
	cv_results = df.mean().tolist()
	return cv_results


# PREDICTING LABELS GIVEN A CLASSIFIER

def predictLabels(reviewSamples, classifier):
	# return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))
	return classifier.classify_many(list(map(lambda t: t[0], reviewSamples)))



def predictLabel(reviewSample, classifier):
	return classifier.classify(toFeatureVector(preProcess(reviewSample)))

# MAIN

# loading reviews
rawData = []		  # the filtered data from the dataset file (should be 21000 samples)
preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)
trainData = []		# the training data as a percentage of the total dataset (currently 80%, or 16800 samples)
testData = []		 # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)

# the output classes
fakeLabel = 'fake'
realLabel = 'real'

# references to the data files
reviewPath = 'amazon_reviews.txt'

## Do the actual stuff
# We parse the dataset and put it in a raw data list
print("Now %d rawData, %d trainData, %d testData" % (len(rawData), len(trainData), len(testData)),
	  "Preparing the dataset...",sep='\n')
loadData(reviewPath) 
# We split the raw dataset into a set of training data and a set of test data (80/20)
print("Now %d rawData, %d trainData, %d testData" % (len(rawData), len(trainData), len(testData)),
	  "Preparing training and test data...",sep='\n')
splitData(0.8)
# We print the number of training samples and the number of features
print("Now %d rawData, %d trainData, %d testData" % (len(rawData), len(trainData), len(testData)),
	  "Training Samples: ", len(trainData), "Features: ", len(featureDict), sep='\n')

precision,recall,f1,accuracy = crossValidate(trainData, 10)
print('precision',precision)
print('recall',recall)
print('f1',f1)
print('accuracy',accuracy)


'''
Training Classifier...
predicting
(0.7591991341991342, 0.7567676226212812, 0.7550126293671905, None)
0.7553571428571428
Training Classifier...
predicting
(0.7730569574621485, 0.7683748134563683, 0.7674371432262225, None)
0.768452380952381
Training Classifier...
predicting
(0.7716685209776406, 0.7682032384934273, 0.767188999040614, None)
0.7678571428571429
Training Classifier...
predicting
(0.7792553191489362, 0.7761904761904762, 0.7755747061097342, None)
0.7761904761904762
Training Classifier...
predicting
(0.7777304456774656, 0.7749115566037736, 0.7739298576125939, None)
0.7744047619047619
Training Classifier...
predicting
(0.7773147607240375, 0.7725840090399838, 0.7724888240124076, None)
0.7738095238095238
Training Classifier...
predicting
(0.803847367661948, 0.8015735219707718, 0.8008831657703002, None)
0.8011904761904762
Training Classifier...
predicting
(0.7938906963376697, 0.7921866137266024, 0.7909077789412136, None)
0.7910714285714285
Training Classifier...
predicting
(0.7824592804725256, 0.7797767884369091, 0.7800433765390986, None)
0.780952380952381
Training Classifier...
predicting
(0.7814112011790715, 0.7772546795235393, 0.7773585120754222, None)
0.7785714285714286

'''