{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install python-crfsuite\n",
    "# !pip -q install category_encoders\n",
    "# !pip -q install flair\n",
    "# !pip -q install textstat\n",
    "# !pip -q install pyspellchecker\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 11:40:04.252305 139852714387264 file_utils.py:40] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-11 11:40:04,285 loading file /home/nehas/.flair/models/en-pos-ontonotes-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "import string\n",
    "import nltk, re, math\n",
    "from nltk import tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "import collections, itertools\n",
    "from spellchecker import SpellChecker\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "flatten = itertools.chain.from_iterable\n",
    "TAGGER = SequenceTagger.load('pos')\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model/model.crf.tagger')\n",
    "POS_DICTIONARY = {}\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "punct = set(string.punctuation)\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "SPELL = SpellChecker()\n",
    "SPELL.word_frequency.load_words([\"'s\", \"'m\", \"'re\", \"'ll\" , \"'ve\", \"'t\", \"'d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'B': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def tokenNormalizeText(text):\n",
    "    #print \"original:\", text\n",
    "    # Remove Emails\n",
    "#     text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "#     # Remove website links\n",
    "#     text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove distracting single quotes\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    # Remove distracting double quotes\n",
    "    text = re.sub(r'\\\"', \"\", text)\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # word normalisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'/\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'/\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n",
    "    # normalisation\n",
    "    text = re.sub(r\"(\\S)\\1\\1+\",r\"\\1\\1\\1\", text)\n",
    "    #tokenising\n",
    "    \n",
    "    tokens = list(flatten([re.split(r\"\\s+\",t) for t in re.split('(\\d+)',text)]))\n",
    "    tokens = [re.sub(r'[^A-Za-z]+','',t) for t in tokens]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in ' ']# and len(t) > 2]\n",
    "    tokens = [w for w in tokens if w not in stopwords ]\n",
    "    tokens = [lemmatize(token, tag) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(' '.join(tokens)))]\n",
    "#     tokens = [str(porter.stem(w)) for w in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def normalizeText(text):\n",
    "    # just in case\n",
    "    s = str(text).lower()\n",
    "#     s = s.lower()\n",
    "#     s = re.sub('\\s\\W',' ',s) #  hyphens, apostrophes\n",
    "#     s = re.sub('\\W\\s',' ',s)\n",
    "#     s = re.sub('\\s+',' ',s) # double spaces\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiagActFeatures(sentence):\n",
    "    features = []\n",
    "    if len(sentence.split())>=2:\n",
    "        tagged_sent = tag.pos_tag(nltk.word_tokenize(sentence))\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('TOKEN_'+tagset[0])\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('POS_'+tagset[1])\n",
    "        for words in sentence.split():\n",
    "            features.append(words)\n",
    "        features.append('/')\n",
    "    return [[features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flairPOSTag(words):\n",
    "    if words in POS_DICTIONARY:\n",
    "        return POS_DICTIONARY[words]\n",
    "    else:\n",
    "        postags = []\n",
    "        text = str(words).replace(',', ', ').replace('.', '. ')\n",
    "        sentence = Sentence(text, use_tokenizer=True)\n",
    "        TAGGER.predict(sentence)\n",
    "        for token in sentence:\n",
    "            pos = token.get_tag('pos').value\n",
    "            postags.append(pos)\n",
    "        POS_DICTIONARY[words] = postags\n",
    "        return postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFMeasure(text):\n",
    "    tagged = flairPOSTag(text)\n",
    "\n",
    "    freq = {}\n",
    "    freq['noun'] = 0\n",
    "    freq['adj'] = 0\n",
    "    freq['prep'] = 0\n",
    "    freq['art'] = 0\n",
    "    freq['pron'] = 0\n",
    "    freq['verb'] = 0\n",
    "    freq['adv'] = 0\n",
    "    freq['int'] = 0\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(tagged)):\n",
    "        pos = tagged[i]\n",
    "        if pos in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            freq['noun'] += 1\n",
    "        elif pos in ['JJ', 'JJR', 'JJS']:\n",
    "            freq['adj'] += 1\n",
    "        elif pos in ['IN']:\n",
    "            freq['prep'] += 1\n",
    "        elif pos in ['DET', 'DT', 'PDT', 'WDT']:\n",
    "            freq['art'] += 1\n",
    "        elif pos in ['PRP', 'PRP$', 'WP', 'WP$']:\n",
    "            freq['pron'] += 1\n",
    "        elif pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            freq['verb'] += 1\n",
    "        elif pos in ['RB', 'RBR', 'RBS', 'WRB']:\n",
    "            freq['adv'] += 1\n",
    "        elif pos in ['UH']:\n",
    "            freq['int'] += 1\n",
    "\n",
    "        if pos not in ['$', \"'\", '(', ')', ',', '-', '.', ':', 'SYM', \"''\", '``']:\n",
    "            count += 1\n",
    "\n",
    "    for key in freq:\n",
    "        freq[key] = (freq[key] / count) * 100\n",
    "\n",
    "    fmeasure = 0.5 * ( (freq['noun'] + freq['adj'] + freq['prep'] + freq['art']) - (freq['pron'] + freq['verb'] + freq['adv'] + freq['int']) + 100 )\n",
    "\n",
    "    return fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POSFeatures(text):\n",
    "    pos_tags = flairPOSTag(text)\n",
    "    return ' '.join(pos_tags)\n",
    "\n",
    "def POSTaggedFeatures(text):\n",
    "    pos_tags = flairPOSTag(text)\n",
    "    tagged_sent = []\n",
    "\n",
    "    cleaned_text = str(text).replace(',', ', ').replace('.', '. ')\n",
    "    sentence = Sentence(cleaned_text, use_tokenizer=True)\n",
    "\n",
    "    for i in range(len(pos_tags)):\n",
    "        tagged_sent.append(sentence[i].text + '_' + pos_tags[i])\n",
    "\n",
    "    return ' '.join(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textTokenizer(text):\n",
    "    text = re.sub(\"[/%-._]\", \" \", text)\n",
    "    text = re.sub(\"[,()!;$?:~*]\",\"\", text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\" '\", '')\n",
    "    text = text.replace(\"' \", '')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGenderPreferentialFeatures(text):\n",
    "    f = []\n",
    "    for i in range(10):\n",
    "        f.append(0)\n",
    "    for word in textTokenizer(text):\n",
    "        word = word.lower()\n",
    "        if word.endswith(('able')):\n",
    "            f[0] += 1\n",
    "        elif word.endswith(('al')):\n",
    "            f[1] += 1\n",
    "        elif word.endswith(('ful')):\n",
    "            f[2] += 1\n",
    "        elif word.endswith(('ible')):\n",
    "            f[3] += 1\n",
    "        elif word.endswith(('ic')):\n",
    "            f[4] += 1\n",
    "        elif word.endswith(('ive')):\n",
    "            f[5] += 1\n",
    "        elif word.endswith(('less')):\n",
    "            f[6] += 1\n",
    "        elif word.endswith(('ly')):\n",
    "            f[7] += 1\n",
    "        elif word.endswith(('ous')):\n",
    "            f[8] += 1\n",
    "        if word in ['sorry', 'penitent', 'contrite', 'repentant', 'remorseful', 'regretful', 'compunctious', 'touched', 'melted', 'sorrowful', 'apologetic', 'softened'\n",
    "                      'sad', 'greived', 'mournful']:\n",
    "            f[9] += 1\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFactorAnalysis(text):\n",
    "    f = []\n",
    "    words_in_factor = []\n",
    "    # Conversation\n",
    "    words_in_factor.append(['know', 'people', 'think', 'person', 'tell', 'feel', 'friends', 'talk', 'new', 'talking', 'mean', 'ask', 'understand', \n",
    "                    'feelings', 'care', 'thinking', 'friend', 'relationship', 'realize', 'question', 'answer', 'saying'])\n",
    "    # AtHome\n",
    "    words_in_factor.append(['woke', 'home', 'sleep', 'today', 'eat', 'tired', 'wake', 'watch', 'watched', 'dinner', 'ate', 'bed', 'day', 'house', 'tv', 'early', 'boring', 'yesterday', 'watching', 'sit'])\n",
    "    # Family\n",
    "    words_in_factor.append(['years', 'family', 'mother', 'children', 'father', 'kids', 'parents', 'old', 'year', 'child', 'son', 'married', 'sister', 'dad', 'brother', 'moved', 'age', 'young', \n",
    "                            'months', 'three', 'wife', 'living', 'college', 'four', 'high', 'five', 'died', 'six', 'baby', 'boy', 'spend', 'christmas'])\n",
    "    # Time\n",
    "    words_in_factor.append(['friday', 'saturday', 'weekend', 'week', 'sunday', 'night', 'monday', 'tuesday', 'thursday', 'wednesday', 'morning', 'tomorrow', 'tonight', 'evening', 'days', \n",
    "                            'afternoon', 'weeks', 'hours', 'july', 'busy', 'meeting', 'hour', 'month', 'june'])\n",
    "    # Work\n",
    "    words_in_factor.append(['work', 'working', 'job', 'trying', 'right', 'met', 'figure', 'meet', 'start', 'better', 'starting', 'try', 'worked', 'idea'])\n",
    "    # PastActions\n",
    "    words_in_factor.append(['said', 'asked', 'told', 'looked', 'walked', 'called', 'talked', 'wanted', 'kept', 'took', 'sat', 'gave', 'knew', 'felt', 'turned', 'stopped', 'saw', 'ran', 'tried', \n",
    "                            'picked', 'left', 'ended'])\n",
    "    # Games\n",
    "    words_in_factor.append(['game', 'games', 'team', 'win', 'play', 'played', 'playing', 'won', 'season', 'beat', 'final', 'two', 'hit', 'first', 'video', 'second', 'run', 'star', 'third', 'shot', \n",
    "                            'table', 'round', 'ten', 'chance', 'club', 'big', 'straight'])\n",
    "    # Internet\n",
    "    words_in_factor.append(['site', 'email', 'page', 'please', 'website', 'web', 'post', 'link', 'check', 'blog', 'mail', 'information', 'free', 'send', 'comments', 'comment', 'using', \n",
    "                            'internet', 'online', 'name', 'service', 'list', 'computer', 'add', 'thanks', 'update', 'message'])\n",
    "    # Location\n",
    "    words_in_factor.append(['street', 'place', 'town', 'road', 'city', 'walking', 'trip', 'headed', 'front', 'car', 'beer', 'apartment', 'bus', 'area', 'park', 'building', 'walk', 'small', 'places', \n",
    "                            'ride', 'driving', 'looking', 'local', 'sitting', 'drive', 'bar', 'bad', 'standing', 'floor', 'weather', 'beach', 'view'])\n",
    "    # Fun\n",
    "    words_in_factor.append(['fun', 'im', 'cool', 'mom', 'summer', 'awesome', 'lol', 'stuff', 'pretty', 'ill', 'mad', 'funny', 'weird'])\n",
    "    # Food/Clothes\n",
    "    words_in_factor.append(['food', 'eating', 'weight', 'lunch', 'water', 'hair', 'life', 'white', 'wearing', 'color', 'ice', 'red', 'fat', 'body', 'black', 'clothes', 'hot', 'drink', 'wear', \n",
    "                            'blue', 'minutes', 'shirt', 'green', 'coffee', 'total', 'store', 'shopping'])\n",
    "    # Poetic\n",
    "    words_in_factor.append(['eyes', 'heart', 'soul', 'pain', 'light', 'deep', 'smile', 'dreams', 'dark', 'hold', 'hands', 'head', 'hand', 'alone', 'sun', 'dream', 'mind', 'cold', 'fall', 'air', \n",
    "                            'voice', 'touch', 'blood', 'feet', 'words', 'hear', 'rain', 'mouth'])\n",
    "    # Books/Movies\n",
    "    words_in_factor.append(['book', 'read', 'reading', 'books', 'story', 'writing', 'written', 'movie', 'stories', 'movies', 'film', 'write', 'character', 'fact', 'thoughts', \n",
    "                            'title', 'short', 'take', 'wrote'])\n",
    "    # Religion\n",
    "    words_in_factor.append(['god', 'jesus', 'lord', 'church', 'earth', 'world', 'word', 'lives', 'power', 'human', 'believe', 'given', 'truth', 'thank', 'death', 'evil', 'own', 'peace', \n",
    "                            'speak', 'bring', 'truly'])\n",
    "    # Romance\n",
    "    words_in_factor.append(['forget', 'forever', 'remember', 'gone', 'true', 'face', 'spent', 'times', 'love', 'cry', 'hurt', 'wish', 'loved'])\n",
    "    # Swearing\n",
    "    words_in_factor.append(['shit', 'fuck', 'fucking', 'ass', 'bitch', 'damn', 'hell', 'sucks', 'stupid', 'hate', 'drunk', 'crap', 'kill', 'guy', 'gay', 'kid', 'sex', 'crazy', 'cunt', 'nigger', 'nigga', \n",
    "                            'asshole', 'pussy', 'dick', 'dickhead', 'faggot', 'fag'])\n",
    "    # Politics\n",
    "    words_in_factor.append(['bush', 'president', 'iraq', 'kerry', 'war', 'american', 'political', 'states', 'america', 'country', 'government', 'john', 'national', 'news', 'state', 'support', \n",
    "                            'issues', 'article', 'michael', 'bill', 'report', 'public', 'issue', 'history', 'party', 'york', 'law', 'major', 'act', 'fight', 'poor'])\n",
    "    # Music\n",
    "    words_in_factor.append(['music', 'songs', 'song', 'band', 'cd', 'rock', 'listening', 'listen', 'show', 'favorite', 'radio', 'sound', 'heard', 'shows', 'sounds', 'amazing', 'dance'])\n",
    "    # School\n",
    "    words_in_factor.append(['school', 'teacher', 'class', 'study', 'test', 'finish', 'english', 'students', 'period', 'paper', 'pass'])\n",
    "    # Business\n",
    "    words_in_factor.append(['system', 'based', 'process', 'business', 'control', 'example', 'personal', 'experience', 'general'])\n",
    "    # Positive\n",
    "    words_in_factor.append(['absolutely', 'abundance', 'ace', 'active', 'admirable', 'adore', 'agree', 'amazing', 'appealing', 'attraction', 'bargain', 'beaming', 'beautiful', 'best', 'better', \n",
    "                            'boost', 'breakthrough', 'breeze', 'brilliant', 'brimming', 'charming', 'clean', 'clear', 'colorful', 'compliment', 'confidence', 'cool', 'courteous', 'cuddly', \n",
    "                            'dazzling', 'delicious', 'delightful', 'dynamic', 'easy', 'ecstatic', 'efficient', 'enhance', 'enjoy', 'enormous', 'excellent', 'exotic', 'expert', 'exquisite', \n",
    "                            'flair', 'free', 'generous', 'genius', 'great', 'graceful', 'heavenly', 'ideal', 'immaculate', 'impressive', 'incredible', 'inspire', 'luxurious', 'outstanding', \n",
    "                            'royal', 'speed', 'splendid', 'spectacular', 'superb', 'sweet', 'sure', 'supreme', 'terrific', 'treat', 'treasure', 'ultra', 'unbeatable', 'ultimate', 'unique', 'wow', 'zest'])\n",
    "    # Negative\n",
    "    words_in_factor.append(['wrong', 'stupid', 'bad', 'evil', 'dumb', 'foolish', 'grotesque', 'harm', 'fear', 'horrible', 'idiot', 'lame', 'mean', 'poor', 'heinous', 'hideous', 'deficient', \n",
    "                            'petty', 'awful', 'hopeless', 'fool', 'risk', 'immoral', 'risky', 'spoil', 'spoiled', 'malign', 'vicious', 'wicked', 'fright', 'ugly', 'atrocious', 'moron', 'hate', \n",
    "                            'spiteful', 'meager', 'malicious', 'lacking'])\n",
    "    # Emotion\n",
    "    words_in_factor.append(['aggressive', 'alienated', 'angry', 'annoyed', 'anxious', 'careful', 'cautious', 'confused', 'curious', 'depressed', 'determined', 'disappointed', 'discouraged', \n",
    "                            'disgusted', 'ecstatic', 'embarrassed', 'enthusiastic', 'envious', 'excited', 'exhausted', 'frightened', 'frustrated', 'guilty', 'happy', 'helpless', 'hopeful', \n",
    "                            'hostile', 'humiliated', 'hurt', 'hysterical', 'innocent', 'interested', 'jealous', 'lonely', 'mischievous', 'miserable', 'optimistic', 'paranoid', 'peaceful', \n",
    "                            'proud', 'puzzled', 'regretful', 'relieved', 'sad', 'satisfied', 'shocked', 'shy', 'sorry', 'surprised', 'suspicious', 'thoughtful', 'undecided', 'withdrawn'])\n",
    "\n",
    "    for i in range(len(words_in_factor)):\n",
    "        f.append(0)\n",
    "    for word in textTokenizer(text):\n",
    "        word = word.lower()\n",
    "        for i in range(len(words_in_factor)):\n",
    "            if word in words_in_factor[i]:\n",
    "                f[i] += 1\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTextStatInfo(text):\n",
    "    le_c = textstat.lexicon_count(text, removepunct=True)\n",
    "    ts = textstat.text_standard(text, float_output=True)\n",
    "\n",
    "    return le_c, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNumberOfIncorrectSpelling(text):\n",
    "    tokens = textTokenizer(text)\n",
    "\n",
    "    misspelled = SPELL.unknown(tokens)\n",
    "\n",
    "    #for i in range(len(misspelled)):\n",
    "    #    misspelled[i] = ''.join(e for e in misspelled[i] if e.isalnum())\n",
    "\n",
    "    misspelled.discard('')\n",
    "    return len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_DICTIONARY = np.ndarray.tolist(np.load(\"data/pos_dict.npy\",allow_pickle=True))\n",
    "df = pd.read_csv(\"train_test/training.csv\",names=['text','character','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_norm'] = df.apply(lambda x: normalizeText(str(x.text)),axis = 1)\n",
    "df['token_text_norm'] = df.apply(lambda x: tokenNormalizeText(str(x.text)),axis = 1)\n",
    "df['POS'] = df.apply(lambda x: POSFeatures(x.text), axis = 1)\n",
    "df['POS_tagged'] = df.apply(lambda x: POSTaggedFeatures(x.text), axis = 1)\n",
    "df['f_measure'] = df.apply(lambda x: calcFMeasure(x.text), axis=1)\n",
    "# np.save(\"data/pos_dict.npy\", POS_DICTIONARY)\n",
    "df['word_count'] = df.apply(lambda x: len(textTokenizer(str(x.text))) , axis=1)\n",
    "df['length'] = df.apply(lambda x: len(str(x.text)), axis = 1)\n",
    "df['gf'] = df.apply(lambda x: GetGenderPreferentialFeatures(str(x.text)), axis = 1)\n",
    "df[[\"GPF0\",\"GPF1\",\"GPF2\",\"GPF3\",\"GPF4\",\"GPF5\",\"GPF6\",\"GPF7\",\"GPF8\",\"GPF9\"]] = pd.DataFrame(df.gf.values.tolist(), index= df.index)\n",
    "df['fa'] = df.apply(lambda x: GetFactorAnalysis(str(x.text)), axis = 1)\n",
    "df[[\"F0\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\",\"F13\",\"F14\",\"F15\",\"F16\",\"F17\",\"F18\",\"F19\",\"F20\",\"F21\",\"F22\"]] = pd.DataFrame(df.fa.values.tolist(), index= df.index)\n",
    "df['diag_act'] = df.apply(lambda x: [tagger.tag(xseq) for xseq in createDiagActFeatures(str(x.text))][0][0], axis=1)\n",
    "mostCommonDiagAct = collections.Counter(df.diag_act.values.tolist()).most_common(8)\n",
    "mostCommonDiagAct = [i[0] for i in mostCommonDiagAct]\n",
    "def helper(a):\n",
    "    if a not in mostCommonDiagAct:\n",
    "        return 'othr'\n",
    "    else: return a\n",
    "df['diag_act'] = df.apply(lambda x: helper(x.diag_act),axis=1)\n",
    "df['diag_act'].value_counts()\n",
    "df = pd.get_dummies(df, columns=[\"diag_act\"], prefix=[\"diag_act\"])\n",
    "df['LE_C'] = df.apply(lambda x: GetTextStatInfo(str(x.text))[0], axis = 1)\n",
    "df['TS'] = df.apply(lambda x: GetTextStatInfo(str(x.text))[1], axis = 1)\n",
    "df['mispelled'] = df.apply(lambda x: GetNumberOfIncorrectSpelling(str(x.text)), axis = 1)\n",
    "df['gender'] = [1 if x =='male' else -1 for x in df['gender']] \n",
    "\n",
    "\n",
    "df.drop(['gf','fa','character'],axis=1).to_csv('data/train_gender.csv',index=False)\n",
    "df.drop(['gf','fa','gender'],axis=1).to_csv('data/train_character.csv',index=False)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('data/training_data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.drop(['gf','fa','character'],axis=1).to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start POS Mining\n",
      "Total Amount of Documents: 10113\n",
      "Total Amount of Unique POS: 19\n",
      "Minimum Support: 3033\n",
      "Minimum Adherence: 0.20\n",
      "Stopped at k = 7\n",
      "Extracted POS Patterns: 222\n"
     ]
    }
   ],
   "source": [
    "from MinePOSPats import MinePOSPats\n",
    "pos_list = df['POS'].values.tolist()\n",
    "mine_obj = MinePOSPats(pos_list, 0.3, 0.2)\n",
    "pos_pats = mine_obj.MinePOSPats()\n",
    "\n",
    "# Write POS Patterns to Text\n",
    "with open('data/POSPatterns.txt', 'w') as file:\n",
    "    patterns = []\n",
    "    for pos_pat in pos_pats:\n",
    "        pattern = ' '.join(pos_pat)\n",
    "        patterns.append(pattern)\n",
    "    file.write('\\n'.join(patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_test/test.csv\",names=['text','character','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_norm'] = df.apply(lambda x: normalizeText(str(x.text)),axis = 1)\n",
    "df['token_text_norm'] = df.apply(lambda x: tokenNormalizeText(str(x.text)),axis = 1)\n",
    "df['POS'] = df.apply(lambda x: POSFeatures(x.text), axis = 1)\n",
    "df['POS_tagged'] = df.apply(lambda x: POSTaggedFeatures(x.text), axis = 1)\n",
    "df['f_measure'] = df.apply(lambda x: calcFMeasure(x.text), axis=1)\n",
    "# np.save(\"data/pos_dict.npy\", POS_DICTIONARY)\n",
    "df['word_count'] = df.apply(lambda x: len(textTokenizer(str(x.text))) , axis=1)\n",
    "df['length'] = df.apply(lambda x: len(str(x.text)), axis = 1)\n",
    "df['gf'] = df.apply(lambda x: GetGenderPreferentialFeatures(str(x.text)), axis = 1)\n",
    "df[[\"GPF0\",\"GPF1\",\"GPF2\",\"GPF3\",\"GPF4\",\"GPF5\",\"GPF6\",\"GPF7\",\"GPF8\",\"GPF9\"]] = pd.DataFrame(df.gf.values.tolist(), index= df.index)\n",
    "df['fa'] = df.apply(lambda x: GetFactorAnalysis(str(x.text)), axis = 1)\n",
    "df[[\"F0\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\",\"F13\",\"F14\",\"F15\",\"F16\",\"F17\",\"F18\",\"F19\",\"F20\",\"F21\",\"F22\"]] = pd.DataFrame(df.fa.values.tolist(), index= df.index)\n",
    "df['diag_act'] = df.apply(lambda x: [tagger.tag(xseq) for xseq in createDiagActFeatures(str(x.text))][0][0], axis=1)\n",
    "mostCommonDiagAct = collections.Counter(df.diag_act.values.tolist()).most_common(8)\n",
    "mostCommonDiagAct = [i[0] for i in mostCommonDiagAct]\n",
    "def helper(a):\n",
    "    if a not in mostCommonDiagAct:\n",
    "        return 'othr'\n",
    "    else: return a\n",
    "df['diag_act'] = df.apply(lambda x: helper(x.diag_act),axis=1)\n",
    "df['diag_act'].value_counts()\n",
    "df = pd.get_dummies(df, columns=[\"diag_act\"], prefix=[\"diag_act\"])\n",
    "df['LE_C'] = df.apply(lambda x: GetTextStatInfo(str(x.text))[0], axis = 1)\n",
    "df['TS'] = df.apply(lambda x: GetTextStatInfo(str(x.text))[1], axis = 1)\n",
    "df['mispelled'] = df.apply(lambda x: GetNumberOfIncorrectSpelling(str(x.text)), axis = 1)\n",
    "df['gender'] = [1 if x =='male' else -1 for x in df['gender']] \n",
    "\n",
    "df.drop(['gf','fa','character'],axis=1).to_csv('data/test_gender.csv',index=False)\n",
    "df.drop(['gf','fa','gender'],axis=1).to_csv('data/test_character.csv',index=False)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('data/testing_data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.drop(['gf','fa','character'],axis=1).to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/projects/virt-py3tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'col'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eac27ca4552f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'col'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/projects/virt-py3tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/projects/virt-py3tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'col'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
