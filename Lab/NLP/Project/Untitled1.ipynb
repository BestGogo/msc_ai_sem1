{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from numpy.random import choice\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import expon\n",
    "\n",
    "from Classes.ColumnExtractor import ColumnExtractor\n",
    "from Classes.DenseTransformer import DenseTransformer\n",
    "from Classes.EFS import EFS\n",
    "from Classes.FSC import FSC\n",
    "from Classes.ItemSelector import ItemSelector\n",
    "from Classes.ItemSelectorTF import ItemSelectorTF\n",
    "from Helper.DebugPrint import DebugPrint\n",
    "\n",
    "RANDOMIZER_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def GetFeatures(self, training_data_dict, training_data_classification, vectorizer_pipeline, classifier, feature_selections):\n",
    "        start = time.time()\n",
    "        efs_obj = EFS()\n",
    "\n",
    "        X = vectorizer_pipeline.fit_transform(training_data_dict, training_data_classification)\n",
    "        candidate_feature_indexes = efs_obj.EFS(X, training_data_classification, classifier, feature_selections)\n",
    "\n",
    "        end = time.time()\n",
    "        DebugPrint(\"Time Run = %fs\" % (end - start))\n",
    "\n",
    "        return candidate_feature_indexes\n",
    "    \n",
    "    def BuildClassifierNB(self, training_data_dict, training_data_classification, vocab, word_vocab, nb_type):\n",
    "        ## Build and Train Model ######################\n",
    "        if nb_type == 'tf': \n",
    "            ## TF - 1-GRAM [CHI, IG] [ALL] (0.716 ACC)\n",
    "            pos_vectorizer = CountVectorizer(vocabulary=vocab, analyzer='word', ngram_range=(1, 5), tokenizer=lambda x: x.split(' '), lowercase=False)\n",
    "            text_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, tokenizer=lambda x: x.split(' '))\n",
    "\n",
    "            classifier = MultinomialNB()\n",
    "\n",
    "            features1 = FeatureUnion([\n",
    "                    ('pos', Pipeline([\n",
    "                        ('selector', ItemSelector(key='pos')),\n",
    "                        ('vectorizer', pos_vectorizer),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('text', Pipeline([\n",
    "                        ('selector', ItemSelector(key='tokenized_text')),\n",
    "                        ('vectorizer', text_vectorizer),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('gpf', Pipeline([\n",
    "                        ('selector', ItemSelector(key='gpf')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('fa', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fa')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            features2 = FeatureUnion([\n",
    "                    ('wordcount', Pipeline([\n",
    "                        ('selector', ItemSelector(key='wordcount')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        ('discretize', KBinsDiscretizer(n_bins = 10, encode='ordinal', strategy='uniform')),\n",
    "                    ])),\n",
    "                    ('fmeasure', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fmeasure')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        ('discretize', KBinsDiscretizer(n_bins = 10, encode='ordinal', strategy='uniform')),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            reducer_features = self.GetFeatures(training_data_dict, training_data_classification, features1, classifier, [FSC.CHI, FSC.IG, FSC.MI, FSC.CE, FSC.WOE])\n",
    "            reducer = ColumnExtractor(cols=reducer_features)\n",
    "\n",
    "            text_clf = Pipeline([\n",
    "                ('features', FeatureUnion([\n",
    "                    ('pipeline', Pipeline([\n",
    "                        ('features', features1),\n",
    "                        ('reducer', reducer),\n",
    "                        ('scaler', MaxAbsScaler()),\n",
    "                    ])),\n",
    "                    ('pipeline2', Pipeline([\n",
    "                        ('features', features2),\n",
    "                        ('scaler', MinMaxScaler()),\n",
    "                    ])),\n",
    "                ])),\n",
    "                ('clf', classifier),\n",
    "            ])\n",
    "        text_clf.fit(training_data_dict, training_data_classification)\n",
    "        ##############################################\n",
    "\n",
    "        feats = text_clf.named_steps['features']\n",
    "        test = feats.transform(training_data_dict)\n",
    "        print('Training Vect Examples')\n",
    "        for (count, feature) in zip(test[0].toarray()[0], vectorizer.get_feature_names()):\n",
    "           print(str(count) + ' ' + feature)\n",
    "        print(zip(test[0], vectorizer.get_feature_names()))\n",
    "        print(test[1])\n",
    "\n",
    "        feats = text_clf.named_steps['features']\n",
    "        test = feats.transform(training_data_dict)\n",
    "        DebugPrint(test[1])\n",
    "        print(test[1].toarray())\n",
    "        return text_clf\n",
    "    def BuildClassifierSVM(self, training_data_dict, training_data_classification, vocab, word_vocab, svm_type):\n",
    "        ## Build and Train Model ######################\n",
    "\n",
    "        if svm_type == 'bool': \n",
    "            # Bool - 0.67\n",
    "            # Kind of sucks\n",
    "            # Reducer sux\n",
    "            pos_vectorizer = CountVectorizer(vocabulary=vocab, analyzer='word', ngram_range=(1, 5), tokenizer=lambda x: x.split(' '), lowercase=False)\n",
    "            text_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), tokenizer=lambda x: x.split(' '), lowercase=True)\n",
    "\n",
    "            classifier = LinearSVC(max_iter=100000, C=0.001, penalty='l2', loss='squared_hinge')\n",
    "            parameters = [\n",
    "                    {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l2'], 'loss': ['hinge', 'squared_hinge']},\n",
    "                    {'C': [0.001, 0.01, 0.1, 1, 10], 'penalty': ['l1'], 'loss': ['squared_hinge'], 'dual': [False]}\n",
    "                ]\n",
    "            final_classifier = GridSearchCV(classifier, parameters, cv=5, n_jobs=7)\n",
    "\n",
    "            features1 = FeatureUnion([\n",
    "                    ('pos', Pipeline([\n",
    "                        ('selector', ItemSelector(key='pos')),\n",
    "                        ('vectorizer', pos_vectorizer),\n",
    "                        ('binarize', Binarizer()),\n",
    "                    ])),\n",
    "                    ('text', Pipeline([\n",
    "                        ('selector', ItemSelector(key='tokenized_text')),\n",
    "                        ('vectorizer', text_vectorizer),\n",
    "                        ('binarize', Binarizer()),\n",
    "                    ])),\n",
    "                    ('gpf', Pipeline([\n",
    "                        ('selector', ItemSelector(key='gpf')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('binarize', Binarizer()),\n",
    "                    ])),\n",
    "                    ('fa', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fa')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('binarize', Binarizer()),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            features2 = FeatureUnion([\n",
    "                    ('wordcount', Pipeline([\n",
    "                        ('selector', ItemSelector(key='wordcount')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        ('discretize', KBinsDiscretizer(n_bins = 2, encode='ordinal', strategy='uniform')),\n",
    "                    ])),\n",
    "                    ('fmeasure', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fmeasure')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        #('discretize', KBinsDiscretizer(n_bins = 2, encode='ordinal', strategy='uniform')),\n",
    "                        ('binarize', Binarizer(threshold=50)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            #reducer_features = self.GetFeatures(training_data_dict, training_data_classification, features1, classifier, [FSC.CHI, FSC.IG, FSC.MI, FSC.CE, FSC.WOE])\n",
    "            #reducer = ColumnExtractor(cols=reducer_features)\n",
    "\n",
    "            text_clf = Pipeline([\n",
    "                ('features', FeatureUnion([\n",
    "                    ('pipeline', Pipeline([\n",
    "                        ('features', features1),\n",
    "                        #('reducer', reducer),\n",
    "                    ])),\n",
    "                    ('pipeline2', Pipeline([\n",
    "                        ('features', features2),\n",
    "                    ])),\n",
    "                ])),\n",
    "                ('clf', final_classifier),\n",
    "            ])\n",
    "        elif svm_type == 'discrete': \n",
    "            # Discrete - 2GRAM 0.70 ACC NO REDUC\n",
    "            # Normalizer Performs Better Than MinMaxScaler\n",
    "            # L2 norm performs better than L1 normalization\n",
    "            # 1-Gram vs 2-Gram doen't matter - TO DO\n",
    "            pos_vectorizer = CountVectorizer(vocabulary=vocab, analyzer='word', ngram_range=(1, 5), tokenizer=lambda x: x.split(' '), lowercase=False)\n",
    "            text_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, tokenizer=lambda x: x.split(' '))\n",
    "\n",
    "            classifier = LinearSVC(max_iter=100000)\n",
    "            parameters = [\n",
    "                    {'C': [0.1, 1, 2, 3, 4, 5]},\n",
    "                ]\n",
    "            final_classifier = GridSearchCV(classifier, parameters, cv=5, n_jobs=7)\n",
    "\n",
    "            features1 = FeatureUnion([\n",
    "                    ('pos', Pipeline([\n",
    "                        ('selector', ItemSelector(key='pos')),\n",
    "                        ('vectorizer', pos_vectorizer),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                    ('text', Pipeline([\n",
    "                        ('selector', ItemSelector(key='tokenized_text')),\n",
    "                        ('vectorizer', text_vectorizer),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                    ('gpf', Pipeline([\n",
    "                        ('selector', ItemSelector(key='gpf')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                    ('fa', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fa')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            features2 = FeatureUnion([\n",
    "                    ('wordcount', Pipeline([\n",
    "                        ('selector', ItemSelector(key='wordcount')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                    ('fmeasure', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fmeasure')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                        ('scaler', Normalizer(norm='l2')),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            #reducer_features = self.GetFeatures(training_data_dict, training_data_classification, features1, classifier, [FSC.CHI, FSC.IG])\n",
    "            #reducer = ColumnExtractor(cols=reducer_features)\n",
    "\n",
    "            text_clf = Pipeline([\n",
    "                ('features', FeatureUnion([\n",
    "                    ('pipeline', Pipeline([\n",
    "                        ('features', features1),\n",
    "                        #('reducer', reducer),\n",
    "                    ])),\n",
    "                    ('pipeline2', Pipeline([\n",
    "                        ('features', features2),\n",
    "                    ])),\n",
    "                ])),\n",
    "                ('clf', final_classifier),\n",
    "            ])\n",
    "        elif svm_type == 'svc': \n",
    "            # SVC 0.73 linear 1-gram\n",
    "            # No reducer\n",
    "            # Linear and Sigmoid are Good\n",
    "            pos_vectorizer = CountVectorizer(vocabulary=vocab, analyzer='word', ngram_range=(1, 5), tokenizer=lambda x: x.split(' '), lowercase=False)\n",
    "            text_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), lowercase=True, tokenizer=lambda x: x.split(' '))\n",
    "\n",
    "            classifier = SVC(C=1, kernel='linear')\n",
    "            parameters = [\n",
    "                    {'kernel': ['linear'], 'C': [0.1, 1, 2]},\n",
    "                    #{'kernel': ['rbf'], 'C': [0.1, 1, 2], 'gamma': ['auto', 'scale']},\n",
    "                    ##{'kernel': ['poly'], 'C': [0.1, 1, 2], 'gamma': ['auto', 'scale'], 'degree': [2, 3, 4]},\n",
    "                    #{'kernel': ['sigmoid'], 'C': [0.1, 1, 2], 'gamma': ['auto', 'scale']},\n",
    "                ]\n",
    "            final_classifier = GridSearchCV(classifier, parameters, cv=5, n_jobs=7)\n",
    "\n",
    "            features1 = FeatureUnion([\n",
    "                    ('pos', Pipeline([\n",
    "                        ('selector', ItemSelector(key='pos')),\n",
    "                        ('vectorizer', pos_vectorizer),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('text', Pipeline([\n",
    "                        ('selector', ItemSelector(key='tokenized_text')),\n",
    "                        ('vectorizer', text_vectorizer),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('gpf', Pipeline([\n",
    "                        ('selector', ItemSelector(key='gpf')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                    ('fa', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fa')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            features2 = FeatureUnion([\n",
    "                    ('wordcount', Pipeline([\n",
    "                        ('selector', ItemSelector(key='wordcount')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                    ])),\n",
    "                    ('fmeasure', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fmeasure')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            #reducer_features = self.GetFeatures(training_data_dict, training_data_classification, features1, MultinomialNB(), [FSC.CHI, FSC.IG, FSC.MI, FSC.CE, FSC.WOE])\n",
    "            #reducer = ColumnExtractor(cols=reducer_features)\n",
    "\n",
    "            text_clf = Pipeline([\n",
    "                ('features', FeatureUnion([\n",
    "                    ('pipeline', Pipeline([\n",
    "                        ('features', features1),\n",
    "                        #('reducer', reducer),\n",
    "                        ('scaler', MaxAbsScaler()),\n",
    "                    ])),\n",
    "                    ('pipeline2', Pipeline([\n",
    "                        ('features', features2),\n",
    "                        ('scaler', MinMaxScaler()),\n",
    "                    ])),\n",
    "                ])),\n",
    "                ('clf', final_classifier),\n",
    "            ])\n",
    "        \n",
    "        text_clf.fit(training_data_dict, training_data_classification)\n",
    "        \n",
    "        return text_clf\n",
    "    def BuildClassifierSVMR(self, training_data_dict, training_data_classification, vocab, word_vocab, svmr_type):\n",
    "        ## Build and Train Model ######################\n",
    "        \n",
    "        if svmr_type == 'linearsvr':\n",
    "            # LinearSVR - 0.72\n",
    "            # L2 > L1\n",
    "            # TFIDF > TF\n",
    "            pos_vectorizer = CountVectorizer(vocabulary=vocab, analyzer='word', ngram_range=(1, 5), tokenizer=lambda x: x.split(' '), lowercase=False)\n",
    "            text_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), tokenizer=lambda x: x.split(' '), lowercase=True)\n",
    "\n",
    "            classifier = LinearSVR(max_iter=100000)\n",
    "            parameters = [\n",
    "                    {'C': [0.1, 1, 10], 'epsilon': [0, 0.1, 1], 'loss':('epsilon_insensitive', 'squared_epsilon_insensitive')}\n",
    "                ]\n",
    "            final_classifier = GridSearchCV(classifier, parameters, cv=5, n_jobs=7)\n",
    "\n",
    "            features1 = FeatureUnion([\n",
    "                    ('pos', Pipeline([\n",
    "                        ('selector', ItemSelector(key='pos')),\n",
    "                        ('vectorizer', pos_vectorizer),\n",
    "                        ('tf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                    ])),\n",
    "                    ('text', Pipeline([\n",
    "                        ('selector', ItemSelector(key='tokenized_text')),\n",
    "                        ('vectorizer', text_vectorizer),\n",
    "                        ('tf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                    ])),\n",
    "                    ('gpf', Pipeline([\n",
    "                        ('selector', ItemSelector(key='gpf')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                    ])),\n",
    "                    ('fa', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fa')),\n",
    "                        ('toarray', FunctionTransformer(self.GetMultipleGenericArray, validate = False)),\n",
    "                        ('tf', TfidfTransformer(norm='l2', use_idf=True)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            features2 = FeatureUnion([\n",
    "                    ('wordcount', Pipeline([\n",
    "                        ('selector', ItemSelector(key='wordcount')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                    ])),\n",
    "                    ('fmeasure', Pipeline([\n",
    "                        ('selector', ItemSelector(key='fmeasure')),\n",
    "                        ('toarray', FunctionTransformer(self.GetGenericArray, validate = False)),\n",
    "                    ])),\n",
    "                ])\n",
    "\n",
    "            #reducer_features = self.GetFeatures(training_data_dict, training_data_classification, features1, MultinomialNB(), [FSC.CHI])\n",
    "            #reducer = ColumnExtractor(cols=reducer_features)\n",
    "\n",
    "            text_clf = Pipeline([\n",
    "                ('features', FeatureUnion([\n",
    "                    ('pipeline', Pipeline([\n",
    "                        ('features', features1),\n",
    "                        #('reducer', reducer),\n",
    "                    ])),\n",
    "                    ('pipeline2', Pipeline([\n",
    "                        ('features', features2),\n",
    "                        ('scaler', MinMaxScaler()),\n",
    "                    ])),\n",
    "                ])),\n",
    "                ('clf', final_classifier),\n",
    "            ])\n",
    "                    text_clf.fit(training_data_dict, training_data_classification)\n",
    "        #print(final_classifier.best_params_)\n",
    "        ###############################################\n",
    "\n",
    "        return text_clf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
