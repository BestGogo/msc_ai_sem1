{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install python-crfsuite\n",
    "# !pip -q install category_encoders\n",
    "# !pip -q install flair\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')\n",
    "# # !pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 20:00:15.088458 140684598146880 file_utils.py:40] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-11 20:00:15,131 loading file /home/nehas/.flair/models/en-pos-ontonotes-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7ff3a4118400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "import nltk, re, math\n",
    "from nltk import tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections, itertools\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "flatten = itertools.chain.from_iterable\n",
    "TAGGER = SequenceTagger.load('pos')\n",
    "POS_DICTIONARY = {}\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# function to split the data for cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# function for transforming documents into counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# function for encoding categories\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiagActFeatures(sentence):\n",
    "    features = []\n",
    "    if len(sentence.split())>=2:\n",
    "        tagged_sent = tag.pos_tag(nltk.word_tokenize(sentence))\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('TOKEN_'+tagset[0])\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('POS_'+tagset[1])\n",
    "        for words in sentence.split():\n",
    "            features.append(words)\n",
    "        features.append('/')\n",
    "    return [[features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flairGetPOSTag(d_words):\n",
    "    if d_words in POS_DICTIONARY:\n",
    "        return POS_DICTIONARY[d_words]\n",
    "    else:\n",
    "        d = []\n",
    "        preprocessed_text = d_words.replace(',', ', ').replace('.', '. ')\n",
    "        sentence = Sentence(preprocessed_text, use_tokenizer=True)\n",
    "        TAGGER.predict(sentence)\n",
    "        for token in sentence:\n",
    "            pos = token.get_tag('pos').value\n",
    "            d.append(pos)\n",
    "        POS_DICTIONARY[d_words] = d\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTest = createDiagActFeatures(\"I can imagine.\")\n",
    "# yPred = [tagger.tag(xseq) for xseq in createDiagActFeatures(\"I can imagine.\")][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_test/training.csv\",names=['text','character','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10113, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sd                 2798\n",
       "othr               1714\n",
       "qy                 1563\n",
       "qw                 1374\n",
       "sv                 1014\n",
       "qy^d                500\n",
       "^q                  423\n",
       "ad                  382\n",
       "fo_o_fw_\"_by_bc     345\n",
       "Name: diagAct, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagAct'] = df.apply(lambda x: [tagger.tag(xseq) for xseq in createDiagActFeatures(str(x.text))][0][0], axis=1)\n",
    "mostCommonDiagAct = collections.Counter(df.diagAct.values.tolist()).most_common(8)\n",
    "mostCommonDiagAct = [i[0] for i in mostCommonDiagAct]\n",
    "def helper(a):\n",
    "    if a not in mostCommonDiagAct:\n",
    "        return 'othr'\n",
    "    else: return a\n",
    "df['diagAct'] = df.apply(lambda x: helper(x.diagAct),axis=1)\n",
    "df['diagAct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10113, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One hot encoding\n",
    "df = pd.get_dummies(df, columns=[\"diagAct\"], prefix=[\"diagAct\"])\n",
    "# label encoding\n",
    "# encoder = LabelEncoder()\n",
    "# diagAct = encoder.fit_transform(df['diagAct'])\n",
    "# df['diagActEnc'] = diagAct\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "def normalize_text(s):\n",
    "    # just in case\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    \n",
    "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W\\s',' ',s)\n",
    "    \n",
    "    # make sure we didn't introduce any double spaces\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def token_normalize_text(text):\n",
    "    #print \"original:\", text\n",
    "    # Remove Emails\n",
    "#     text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "#     # Remove website links\n",
    "#     text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove distracting single quotes\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    # Remove distracting double quotes\n",
    "    text = re.sub(r'\\\"', \"\", text)\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # word normalisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'/\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'/\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n",
    "    # normalisation\n",
    "    text = re.sub(r\"(\\S)\\1\\1+\",r\"\\1\\1\\1\", text)\n",
    "    #tokenising\n",
    "    \n",
    "    tokens = list(flatten([re.split(r\"\\s+\",t) for t in re.split('(\\d+)',text)]))\n",
    "    tokens = [re.sub(r'[^A-Za-z]+','',t) for t in tokens]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in ' ' and len(t) > 2]\n",
    "    tokens = [w for w in tokens if w not in stopwords ]\n",
    "    tokens = [str(porter.stem(w)) for w in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['text_norm'] = df.apply(lambda x: normalize_text(str(x.text)),axis = 1)\n",
    "df['token_text_norm'] = df.apply(lambda x: token_normalize_text(str(x.text)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFMeasure(text):\n",
    "    tagged = flairGetPOSTag(text)\n",
    "\n",
    "    freq = {}\n",
    "    freq['noun'] = 0\n",
    "    freq['adj'] = 0\n",
    "    freq['prep'] = 0\n",
    "    freq['art'] = 0\n",
    "    freq['pron'] = 0\n",
    "    freq['verb'] = 0\n",
    "    freq['adv'] = 0\n",
    "    freq['int'] = 0\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(tagged)):\n",
    "        pos = tagged[i]\n",
    "        if pos in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            freq['noun'] += 1\n",
    "        elif pos in ['JJ', 'JJR', 'JJS']:\n",
    "            freq['adj'] += 1\n",
    "        elif pos in ['IN']:\n",
    "            freq['prep'] += 1\n",
    "        elif pos in ['DET', 'DT', 'PDT', 'WDT']:\n",
    "            freq['art'] += 1\n",
    "        elif pos in ['PRP', 'PRP$', 'WP', 'WP$']:\n",
    "            freq['pron'] += 1\n",
    "        elif pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            freq['verb'] += 1\n",
    "        elif pos in ['RB', 'RBR', 'RBS', 'WRB']:\n",
    "            freq['adv'] += 1\n",
    "        elif pos in ['UH']:\n",
    "            freq['int'] += 1\n",
    "\n",
    "        if pos not in ['$', \"'\", '(', ')', ',', '-', '.', ':', 'SYM', \"''\", '``']:\n",
    "            count += 1\n",
    "\n",
    "    for key in freq:\n",
    "        freq[key] = (freq[key] / count) * 100\n",
    "\n",
    "    fmeasure = 0.5 * ( (freq['noun'] + freq['adj'] + freq['prep'] + freq['art']) - (freq['pron'] + freq['verb'] + freq['adv'] + freq['int']) + 100 )\n",
    "\n",
    "    return fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['F-Measure'] = df.apply(lambda x: GetFMeasure(x.text_norm), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "for i in df.token_text_norm.values.tolist():\n",
    "    tokens.append(i.split())\n",
    "tokens = [i for i in flatten(tokens)]\n",
    "\n",
    "counterDict = collections.Counter(tokens)\n",
    "meantokenperdiag=np.mean(df.apply(lambda x: len(str(x.token_text_norm).split()),axis=1))\n",
    "total_no_diag = df.shape[0]\n",
    "total_no_of_clean_words = sum(counterDict.values())\n",
    "print(\"Total number of tokens: \", total_no_of_clean_words)\n",
    "print(\"Total number of unique tokens: \", len(counterDict))\n",
    "print(\"Total Number of dialouge: \", total_no_diag)\n",
    "print(\"Mean token length: \", meantokenperdiag)\n",
    "\n",
    "print(\"Most common tokens\",counterDict.most_common(10))\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "print(\"Building feature dict\")\n",
    "featureDict ={}\n",
    "for k,v in counterDict.items():\n",
    "\tfeatureDict[k] = (float(v)/total_no_of_clean_words)*math.log(float(total_no_diag) / v)\n",
    "\n",
    "df_old = df.copy()\n",
    "x_y = []\n",
    "for index, rows in df.iterrows():\n",
    "    token = str(rows['token_text_norm']).split()\n",
    "    f =[]\n",
    "    for vals in range(int(meantokenperdiag)+2):\n",
    "        \n",
    "        if len(token)>=int(meantokenperdiag)+2:\n",
    "            #do not pad\n",
    "            f.append(featureDict[token[vals]])\n",
    "        else:\n",
    "            #pad with 0\n",
    "            if vals < len(token):\n",
    "                f.append(featureDict[token[vals]])\n",
    "            else:\n",
    "                f.append(0.0)\n",
    "    #f.append(diag)\n",
    "    x_y.append(f)\n",
    "\n",
    "df1 = pd.DataFrame(x_y,columns=['tf1','tf2','tf3','tf4','tf5','tf6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df,df1],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"features.csv\",index=False)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"data/features_gender.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df_new['gender'])\n",
    "# x = df_new.drop(['text', 'character','text_norm','token_text_norm','gender', \"tf1\",\"tf2\",\"tf3\",\"tf4\",\"tf5\",\"tf6\"],axis=1)\n",
    "x = df_new.drop(['gender'],axis =1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "print(nb.score(x_test, y_test))\n",
    "\n",
    "# clf = svm.SVC(kernel='linear', C=1)\n",
    "# scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "# scores\n",
    "# clf = svm.SVC(kernel='linear', C=1)\n",
    "# scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "# scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the data into vectors\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 3, stop_words='english')\n",
    "x = vectorizer.fit_transform(df_new['token_text_norm'])\n",
    "df1 = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "# df.drop('text', axis=1, inplace=True)\n",
    "res = pd.concat([df_new.drop(['text', 'character','text_norm','token_text_norm','gender'],axis=1), df1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['gender'])\n",
    "\n",
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(res, y, test_size=0.2)\n",
    "\n",
    "# take a look at the shape of each of these\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "print(nb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "scores                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the data into vectors\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(df['text_norm'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['character'])\n",
    "\n",
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# take a look at the shape of each of these\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train, y_train)\n",
    "\n",
    "print(nb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "scores      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
