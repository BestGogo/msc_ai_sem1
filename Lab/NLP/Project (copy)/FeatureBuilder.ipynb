{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install python-crfsuite\n",
    "# !pip -q install category_encoders\n",
    "# !pip -q install flair\n",
    "# !pip -q install textstat\n",
    "# !pip -q install pyspellchecker\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1213 09:11:05.788856 140292286613312 file_utils.py:40] PyTorch version 1.3.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-13 09:11:05,823 loading file /home/nehas/.flair/models/en-pos-ontonotes-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "import pycrfsuite\n",
    "import string\n",
    "import nltk, re, math\n",
    "from nltk import tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "import collections, itertools\n",
    "from spellchecker import SpellChecker\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "flatten = itertools.chain.from_iterable\n",
    "TAGGER = SequenceTagger.load('pos')\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model/model.crf.tagger')\n",
    "POS_DICTIONARY = {}\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "punct = set(string.punctuation)\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "SPELL = SpellChecker()\n",
    "SPELL.word_frequency.load_words([\"'s\", \"'m\", \"'re\", \"'ll\" , \"'ve\", \"'t\", \"'d\"])\n",
    "from data import factor_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token, tag):\n",
    "    \"\"\"\n",
    "    Use NLTK Lemmatizer to lemmatize \n",
    "    params: token, tags\n",
    "    return lemma\n",
    "    \"\"\"\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'B': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def tokenNormalizeText(text):\n",
    "    \"\"\"\n",
    "    return sentence in Normalized form\n",
    "    features in stopword removal, and lemmatization\n",
    "    params: text, string\n",
    "    return: text, string\n",
    "    \"\"\"\n",
    "    # Remove distracting single quotes\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    # Remove distracting double quotes\n",
    "    text = re.sub(r'\\\"', \"\", text)\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # word normalisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'/\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'/\\\"“\\(])(\\w)\", r\"\\1 \\2\", text)\n",
    "    # normalisation\n",
    "    text = re.sub(r\"(\\S)\\1\\1+\",r\"\\1\\1\\1\", text)\n",
    "    #tokenising\n",
    "    \n",
    "    tokens = list(flatten([re.split(r\"\\s+\",t) for t in re.split('(\\d+)',text)]))\n",
    "    tokens = [re.sub(r'[^A-Za-z]+','',t) for t in tokens]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in ' ']# and len(t) > 2]\n",
    "    tokens = [w for w in tokens if w not in stopwords ]\n",
    "    tokens = [lemmatize(token, tag) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(' '.join(tokens)))]\n",
    "#     tokens = [str(porter.stem(w)) for w in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def normalizeText(text):\n",
    "    \"\"\"\n",
    "    returns tokenize text in its original form\n",
    "    \"\"\"\n",
    "#     s = s.lower()\n",
    "#     s = re.sub('\\s\\W',' ',s) #  hyphens, apostrophes\n",
    "#     s = re.sub('\\W\\s',' ',s)\n",
    "#     s = re.sub('\\s+',' ',s) # double spaces\n",
    "    tokens = nltk.word_tokenize(str(text).lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def textTokenizer(text):\n",
    "    \"\"\"\n",
    "    returns tokenize text with special character parsing\n",
    "    \"\"\"\n",
    "    text = re.sub(\"[/%-._]\", \" \", text)\n",
    "    text = re.sub(\"[,()!;$?:~*]\",\"\", text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\" '\", '')\n",
    "    text = text.replace(\"' \", '')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDiagActFeatures(sentence):\n",
    "    \"\"\"\n",
    "    Diagloue Act Feature generator, feature are of form\n",
    "    TOKEN_TOKEN POS_TOKEN TOKEN1 TOKEN2\n",
    "    params: sentence, string\n",
    "    return: nested feature list\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    if len(sentence.split())>=2:\n",
    "        tagged_sent = tag.pos_tag(nltk.word_tokenize(sentence))\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('TOKEN_'+tagset[0])\n",
    "        for tagset in tagged_sent:\n",
    "            features.append('POS_'+tagset[1])\n",
    "        for words in sentence.split():\n",
    "            features.append(words)\n",
    "        features.append('/')\n",
    "    return [[features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POSTag(words,type=\"flair\"):\n",
    "    \"\"\"\n",
    "    Two types of POS taggers, Flair(https://github.com/zalandoresearch/flair) and NLTK\n",
    "    # Accuracy was merely increased by 0.5 - 1 % using flair\n",
    "    # Inorder to use flair use data/pos_dict.npy\n",
    "    params: words, a sentence string\n",
    "            tagger, type of tagger string\n",
    "    return: postags, list of pos tag for the senetnce\n",
    "    \"\"\"\n",
    "    if type==\"NLTK\":\n",
    "        if words in POS_DICTIONARY:\n",
    "            return POS_DICTIONARY[words]\n",
    "        else:\n",
    "            postags = []\n",
    "            text = str(words).replace(',', ', ').replace('.', '. ')\n",
    "            tagged_sent = tag.pos_tag(nltk.word_tokenize(text))\n",
    "            postags = [tags[1] for tags in tagged_sent]\n",
    "            sentence = Sentence(text, use_tokenizer=True)\n",
    "            POS_DICTIONARY[words] = postags\n",
    "            return postags\n",
    "    if type==\"flair\":\n",
    "        if words in POS_DICTIONARY:\n",
    "            return POS_DICTIONARY[words]\n",
    "        else:\n",
    "            postags = []\n",
    "            text = str(words).replace(',', ', ').replace('.', '. ')\n",
    "            sentence = Sentence(text, use_tokenizer=True)\n",
    "            TAGGER.predict(sentence)\n",
    "            for token in sentence:\n",
    "                pos = token.get_tag('pos').value\n",
    "                postags.append(pos)\n",
    "            POS_DICTIONARY[words] = postags\n",
    "            return postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFMeasure(text):\n",
    "    \"\"\"\n",
    "          Get F measure which is defined as\n",
    "          f = 0.5[(freq.noun+freq.adj+freq.prep+freq.art)-(freq.pron+freq.verb+freq.adv+freq.int)+100]\n",
    "          freq is the frequency\n",
    "          params: text\n",
    "          return: integer score\n",
    "    \"\"\"\n",
    "\n",
    "    tagged = POSTag(text, type='NLTK')\n",
    "\n",
    "    grammar_freq = {'noun':0,'adj':0,'prep':0,'art':0,'pron':0,'verb':0,'adv':0,'int':0}\n",
    "    grammer_type = {\n",
    "                    'noun' : ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "                    'adj' : ['JJ', 'JJR', 'JJS'],\n",
    "                    'prep':['IN'],\n",
    "                    'art':['DET', 'DT', 'PDT', 'WDT'],\n",
    "                    'pron':['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "                    'verb':['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "                    'adv':['RB', 'RBR', 'RBS', 'WRB'],\n",
    "                    'int':['UH']\n",
    "                    }\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(tagged)):\n",
    "        pos = tagged[i]\n",
    "        if pos in grammer_type['noun']:\n",
    "            grammar_freq['noun'] += 1\n",
    "        elif pos in grammer_type['adj']:\n",
    "            grammar_freq['adj'] += 1\n",
    "        elif pos in grammer_type['prep']:\n",
    "            grammar_freq['prep'] += 1\n",
    "        elif pos in grammer_type['art']:\n",
    "            grammar_freq['art'] += 1\n",
    "        elif pos in grammer_type['pron']:\n",
    "            grammar_freq['pron'] += 1\n",
    "        elif pos in grammer_type['verb']:\n",
    "            grammar_freq['verb'] += 1\n",
    "        elif pos in grammer_type['adv']:\n",
    "            grammar_freq['adv'] += 1\n",
    "        elif pos in grammer_type['int']:\n",
    "            grammar_freq['int'] += 1\n",
    "\n",
    "        if pos not in ['$', \"'\", '(', ')', ',', '-', '.', ':', 'SYM', \"''\", '``']:\n",
    "            count += 1\n",
    "\n",
    "    for key in grammar_freq:\n",
    "        grammar_freq[key] = (grammar_freq[key] / count) * 100\n",
    "\n",
    "    fmeasure = 0.5 * ( (grammar_freq['noun'] + grammar_freq['adj'] + grammar_freq['prep'] + grammar_freq['art']) - (grammar_freq['pron'] + grammar_freq['verb'] + grammar_freq['adv'] + grammar_freq['int']) + 100 )\n",
    "\n",
    "    return fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POSFeatures(text):\n",
    "    \"\"\"\n",
    "    return POS tags for given input text sentence string\n",
    "    \"\"\"\n",
    "    pos_tags = POSTag(text, type='NLTK')\n",
    "    return ' '.join(pos_tags)\n",
    "\n",
    "def POSTaggedFeatures(text, type=\"flair\"):\n",
    "    \"\"\"\n",
    "    Gives POS features in form of TOKEN_POS\n",
    "    params: text, a sentence string\n",
    "            type, type of POS tagger to use (flair/NLTK)\n",
    "    return: tagged_sent, a string of form TOKEN_POS\n",
    "    \"\"\"\n",
    "    if type==\"NLTK\":\n",
    "        pos_tags = POSTag(str(text),type=\"NLTK\")\n",
    "        tagged_sent = []\n",
    "\n",
    "        cleaned_text = str(text).replace(',', ', ').replace('.', '. ')\n",
    "    #     sentence = Sentence(cleaned_text, use_tokenizer=True)\n",
    "        sentence = nltk.word_tokenize(cleaned_text)\n",
    "\n",
    "        for i in range(len(pos_tags)):\n",
    "            tagged_sent.append(sentence[i] + '_' + pos_tags[i])\n",
    "        return ' '.join(tagged_sent)\n",
    "    if type==\"flair\":\n",
    "        pos_tags = POSTag(str(text),type=\"flair\")\n",
    "        tagged_sent = []\n",
    "\n",
    "        cleaned_text = str(text).replace(',', ', ').replace('.', '. ')\n",
    "        sentence = Sentence(cleaned_text, use_tokenizer=True)\n",
    "\n",
    "        for i in range(len(pos_tags)):\n",
    "            tagged_sent.append(sentence[i].text + '_' + pos_tags[i])\n",
    "\n",
    "        return ' '.join(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderFavouredFeatures(text):\n",
    "    \"\"\"\n",
    "    Gender Favoured features are count of words \n",
    "    which are specifically ending with certain \n",
    "    characters or sentences have words in them.\n",
    "    param: text, a sentence string\n",
    "    return: f, a feature list of count\n",
    "    \"\"\"\n",
    "    gf = []\n",
    "    word_types = {\n",
    "                    'f0':'able',\n",
    "                    'f1':'al',\n",
    "                    'f2':'ful',\n",
    "                    'f3':'ible',\n",
    "                    'f4':'ic',\n",
    "                    'f5':'ive',\n",
    "                    'f6':'less',\n",
    "                    'f7':'ly',\n",
    "                    'f8':'ous',\n",
    "                    'f9':['sorry', 'penitent', 'contrite', 'repentant', 'remorseful', \n",
    "                        'regretful', 'compunctious', 'touched', 'melted', 'sorrowful',\n",
    "                        'apologetic', 'softened','sad', 'greived', 'mournful']\n",
    "\n",
    "                }\n",
    "    for i in range(10):\n",
    "        gf.append(0)\n",
    "    for word in textTokenizer(str(text).lower()):\n",
    "        for ftype,fword in word_types.items():\n",
    "            if ftype=='f0' and word.endswith((fword)):\n",
    "                gf[0] += 1\n",
    "            elif ftype=='f1' and word.endswith((fword)):\n",
    "                gf[1] += 1\n",
    "            elif ftype=='f2' and word.endswith((fword)):\n",
    "                gf[2] += 1\n",
    "            elif ftype=='f3' and word.endswith((fword)):\n",
    "                gf[3] += 1\n",
    "            elif ftype=='f4' and word.endswith((fword)):\n",
    "                gf[4] += 1\n",
    "            elif ftype=='f5' and word.endswith((fword)):\n",
    "                gf[5] += 1\n",
    "            elif ftype=='f6' and word.endswith((fword)):\n",
    "                gf[6] += 1\n",
    "            elif ftype=='f7' and word.endswith((fword)):\n",
    "                gf[7] += 1\n",
    "            elif ftype=='f8' and word.endswith((fword)):\n",
    "                gf[8] += 1\n",
    "            if ftype=='f9' and word in fword:\n",
    "                gf[9] += 1\n",
    "    return gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = list(factor_analysis.word_type.keys())\n",
    "def factorAnalysis(text):\n",
    "    \"\"\"\n",
    "    finding groups of similar words that tend to occur in similar documents\n",
    "    group of similar words present in factor_analysis->word_tyoe\n",
    "    params: text, a sentence string\n",
    "    return: fa, a list of occurance of counts of given word in sentence\n",
    "    \"\"\"\n",
    "    fa = []    \n",
    "    for i in range(len(factors)):\n",
    "        fa.append(0)\n",
    "    for word in textTokenizer(str(text).lower()):\n",
    "        for  i,fact in enumerate(factors):\n",
    "            if word in factor_analysis.word_type[fact]:\n",
    "                fa[i] += 1\n",
    "    return fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textStatistics(text):\n",
    "    \"\"\"\n",
    "    returns text statistics such as lexicon count and text standard in a tuple\n",
    "    \"\"\"\n",
    "    le_c = textstat.lexicon_count(text, removepunct=True)\n",
    "    ts = textstat.text_standard(text, float_output=True)\n",
    "\n",
    "    return le_c, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countIncorrectWordChars(text):\n",
    "    \"\"\"\n",
    "    return len of word chars which are incorrect or does not exits in english,(combination of several words).\n",
    "    \"\"\"\n",
    "    tokens = textTokenizer(text)\n",
    "\n",
    "    misspelled = SPELL.unknown(tokens)\n",
    "\n",
    "    #for i in range(len(misspelled)):\n",
    "    #    misspelled[i] = ''.join(e for e in misspelled[i] if e.isalnum())\n",
    "\n",
    "    misspelled.discard('')\n",
    "    return len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS_DICTIONARY = np.ndarray.tolist(np.load(\"data/pos_dict.npy\",allow_pickle=True))\n",
    "df = pd.read_csv(\"train_test/training.csv\",names=['text','character','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10113, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_norm'] = df.apply(lambda x: normalizeText(str(x.text)),axis = 1)\n",
    "df['token_text_norm'] = df.apply(lambda x: tokenNormalizeText(str(x.text)),axis = 1)\n",
    "df['POS'] = df.apply(lambda x: POSFeatures(str(x.text)), axis = 1)\n",
    "df['POS_tagged'] = df.apply(lambda x: POSTaggedFeatures(str(x.text),type='NLTK'), axis = 1)\n",
    "df['f_measure'] = df.apply(lambda x: calcFMeasure(str(x.text)), axis=1)\n",
    "np.save(\"data/pos_dict.npy\", POS_DICTIONARY)\n",
    "df['word_count'] = df.apply(lambda x: len(textTokenizer(str(x.text))) , axis=1)\n",
    "df['length'] = df.apply(lambda x: len(str(x.text)), axis = 1)\n",
    "df['gf'] = df.apply(lambda x: genderFavouredFeatures(str(x.text)), axis = 1)\n",
    "# df[[\"GPF0\",\"GPF1\",\"GPF2\",\"GPF3\",\"GPF4\",\"GPF5\",\"GPF6\",\"GPF7\",\"GPF8\",\"GPF9\"]] = pd.DataFrame(df.gf.values.tolist(), index= df.index)\n",
    "df['fa'] = df.apply(lambda x: factorAnalysis(str(x.text)), axis = 1)\n",
    "# df[[\"F0\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\",\"F13\",\"F14\",\"F15\",\"F16\",\"F17\",\"F18\",\"F19\",\"F20\",\"F21\",\"F22\"]] = pd.DataFrame(df.fa.values.tolist(), index= df.index)\n",
    "df['diag_act'] = df.apply(lambda x: [tagger.tag(xseq) for xseq in createDiagActFeatures(str(x.text))][0][0], axis=1)\n",
    "mostCommonDiagAct = collections.Counter(df.diag_act.values.tolist()).most_common(8)\n",
    "mostCommonDiagAct = [i[0] for i in mostCommonDiagAct]\n",
    "def helper(a):\n",
    "    if a not in mostCommonDiagAct:\n",
    "        return 'othr'\n",
    "    else: return a\n",
    "    \n",
    "df['diag_act'] = df.apply(lambda x: helper(x.diag_act),axis=1)\n",
    "act_types=df['diag_act'].value_counts()\n",
    "df = pd.get_dummies(df, columns=[\"diag_act\"], prefix=[\"diag_act\"])\n",
    "df['diag_act']= df[['diag_act_'+act_types[act_types==i].index[0] for i in act_types]].values.tolist()\n",
    "for i in act_types:\n",
    "    df = df.drop(['diag_act_'+act_types[act_types==i].index[0]],axis=1)\n",
    "\n",
    "df['LE_C'] = df.apply(lambda x: textStatistics(str(x.text))[0], axis = 1)\n",
    "df['TS'] = df.apply(lambda x: textStatistics(str(x.text))[1], axis = 1)\n",
    "df['mispelled'] = df.apply(lambda x: countIncorrectWordChars(str(x.text)), axis = 1)\n",
    "df['gender'] = [1 if x =='male' else -1 for x in df['gender']] \n",
    "# encoder = LabelEncoder()\n",
    "# df['character'] = encoder.fit_transform(df['character'])\n",
    "\n",
    "df.drop(['gf','fa','character'],axis=1).to_csv('data/train_gender.csv',index=False)\n",
    "df.drop(['gf','fa','gender'],axis=1).to_csv('data/train_character.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = df.index\n",
    "df_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for cols in list(df_train.columns):\n",
    "    train_dict[cols] = df_train[cols].values.tolist()\n",
    "np.save('data/train_dict.npy', train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN VBD NN .', 'PRP VBZ DT NN , RB . VB IN , VB CC VB DT NN . VB PRP IN PRP .', 'JJ NN VBD RBR IN RB . WP VBZ DT DT . VBG JJ .', 'VBP PRP VBN DT NN . DT NNS .', 'NNP POS NN .', 'JJ NNP . WP VBZ JJ NNP .', 'NNP . . .', 'UH DT , PRP MD RB VB DT . RB VBZ DT NN . MD VB RB IN PRP$ NNS .', 'PRP VBP PRP VBP VBN NN IN NN RB .', 'IN VBG PRP TO NN .']\n",
      "Start POS Mining\n",
      "Total Amount of Documents: 10113\n",
      "Total Amount of Unique POS: 31\n",
      "Minimum Support: 3033\n",
      "Minimum Adherence: 0.20\n",
      "Stopped at k = 6\n",
      "Extracted POS Patterns: 86\n"
     ]
    }
   ],
   "source": [
    "from MinePOSPats import MinePOSPats\n",
    "pos_list = df['POS'].values.tolist()\n",
    "print(pos_list[0:10])\n",
    "mine_obj = MinePOSPats(pos_list, 0.3, 0.2)\n",
    "pos_pats = mine_obj.MinePOSPats()\n",
    "\n",
    "# Write POS Patterns to Text\n",
    "with open('data/POSPatterns.txt', 'w') as file:\n",
    "    patterns = []\n",
    "    for pos_pat in pos_pats:\n",
    "        pattern = ' '.join(pos_pat)\n",
    "        patterns.append(pattern)\n",
    "    file.write('\\n'.join(patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_test/test.csv\",names=['text','character','gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_norm'] = df.apply(lambda x: normalizeText(str(x.text)),axis = 1)\n",
    "df['token_text_norm'] = df.apply(lambda x: tokenNormalizeText(str(x.text)),axis = 1)\n",
    "df['POS'] = df.apply(lambda x: POSFeatures(str(x.text)), axis = 1)\n",
    "df['POS_tagged'] = df.apply(lambda x: POSTaggedFeatures(str(x.text),type='NLTK'), axis = 1)\n",
    "df['f_measure'] = df.apply(lambda x: calcFMeasure(str(x.text)), axis=1)\n",
    "np.save(\"data/pos_dict.npy\", POS_DICTIONARY)\n",
    "df['word_count'] = df.apply(lambda x: len(textTokenizer(str(x.text))) , axis=1)\n",
    "df['length'] = df.apply(lambda x: len(str(x.text)), axis = 1)\n",
    "df['gf'] = df.apply(lambda x: genderFavouredFeatures(str(x.text)), axis = 1)\n",
    "# df[[\"GPF0\",\"GPF1\",\"GPF2\",\"GPF3\",\"GPF4\",\"GPF5\",\"GPF6\",\"GPF7\",\"GPF8\",\"GPF9\"]] = pd.DataFrame(df.gf.values.tolist(), index= df.index)\n",
    "df['fa'] = df.apply(lambda x: factorAnalysis(str(x.text)), axis = 1)\n",
    "# df[[\"F0\",\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"F7\",\"F8\",\"F9\",\"F10\",\"F11\",\"F12\",\"F13\",\"F14\",\"F15\",\"F16\",\"F17\",\"F18\",\"F19\",\"F20\",\"F21\",\"F22\"]] = pd.DataFrame(df.fa.values.tolist(), index= df.index)\n",
    "df['diag_act'] = df.apply(lambda x: [tagger.tag(xseq) for xseq in createDiagActFeatures(str(x.text))][0][0], axis=1)\n",
    "mostCommonDiagAct = collections.Counter(df.diag_act.values.tolist()).most_common(8)\n",
    "mostCommonDiagAct = [i[0] for i in mostCommonDiagAct]\n",
    "def helper(a):\n",
    "    if a not in mostCommonDiagAct:\n",
    "        return 'othr'\n",
    "    else: return a\n",
    "    \n",
    "df['diag_act'] = df.apply(lambda x: helper(x.diag_act),axis=1)\n",
    "act_types=df['diag_act'].value_counts()\n",
    "df = pd.get_dummies(df, columns=[\"diag_act\"], prefix=[\"diag_act\"])\n",
    "df['diag_act']= df[['diag_act_'+act_types[act_types==i].index[0] for i in act_types]].values.tolist()\n",
    "for i in act_types:\n",
    "    df = df.drop(['diag_act_'+act_types[act_types==i].index[0]],axis=1)\n",
    "\n",
    "df['LE_C'] = df.apply(lambda x: textStatistics(str(x.text))[0], axis = 1)\n",
    "df['TS'] = df.apply(lambda x: textStatistics(str(x.text))[1], axis = 1)\n",
    "df['mispelled'] = df.apply(lambda x: countIncorrectWordChars(str(x.text)), axis = 1)\n",
    "df['gender'] = [1 if x =='male' else -1 for x in df['gender']] \n",
    "# encoder = LabelEncoder()\n",
    "# df['character'] = encoder.fit_transform(df['character'])\n",
    "\n",
    "df.drop(['gf','fa','character'],axis=1).to_csv('data/train_gender.csv',index=False)\n",
    "df.drop(['gf','fa','gender'],axis=1).to_csv('data/train_character.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = df.index\n",
    "df_test = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "for cols in list(df_train.columns):\n",
    "    test_dict[cols] = df_test[cols].values.tolist()\n",
    "np.save('data/test_dict.npy', test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
